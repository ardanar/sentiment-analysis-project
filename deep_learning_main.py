#!/usr/bin/env python3
"""
ğŸ­ Ã‡ok Modaliteli Duygu Analizi - Derin Ã–ÄŸrenme Versiyonu

PROJE ÅARTLARÄ°NA UYGUNLUK:
âœ… ANN tabanlÄ± modeller (CNN, LSTM, Multimodal)
âœ… 71,702+ veri Ã¶rneÄŸi (>1,000)
âœ… GÃ¶rÃ¼ntÃ¼: 128x128 piksel
âœ… NLP: 1,000+ kelime, temizlenmiÅŸ metin
âœ… SÄ±nÄ±flandÄ±rma: 2 sÄ±nÄ±f (POSITIVE/NEGATIVE)
âœ… DeÄŸerlendirme metrikleri: Accuracy, Precision, Recall, F1, ROC-AUC
âœ… GÃ¶rsel Ã§Ä±ktÄ±lar: Training curves, Confusion matrix, ROC curves

KullanÄ±lan Modeller:
ğŸ”¸ CNN - GÃ¶rÃ¼ntÃ¼ analizi
ğŸ”¸ LSTM - Metin analizi  
ğŸ”¸ Multimodal - CNN + LSTM birleÅŸimi
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, f1_score
import warnings
warnings.filterwarnings('ignore')

# Deep learning modÃ¼lÃ¼nÃ¼ import et
from deep_learning_models import MultimodalSentimentAnalyzer

def load_dataset(file_path, sample_size=2000):
    """
    Veri setini yÃ¼kle ve ÅŸartlara uygun hale getir
    
    ÅARTLAR:
    - En az 1,000 veri Ã¶rneÄŸi âœ…
    - SÄ±nÄ±flandÄ±rma iÃ§in dengeli daÄŸÄ±lÄ±m âœ…
    - NLP: En az 1,000 kelime âœ…
    """
    print("ğŸ“Š VERÄ° SETÄ° YÃœKLEME VE KONTROL")
    print("=" * 50)
    
    # Veri setini yÃ¼kle
    print(f"ğŸ“‚ Veri seti yÃ¼kleniyor (Ã¶rnek: {sample_size})...")
    df = pd.read_csv(file_path, nrows=sample_size)
    
    print(f"âœ… YÃ¼klenen veri sayÄ±sÄ±: {len(df):,}")
    print(f"ğŸ“Š SÃ¼tunlar: {list(df.columns)}")
    
    # Åartlara uygunluk kontrolÃ¼
    print("\nğŸ” ÅARTLARA UYGUNLUK KONTROLÃœ:")
    print("-" * 30)
    
    # 1. Veri Ã¶rneÄŸi sayÄ±sÄ±
    if len(df) >= 1000:
        print(f"âœ… Veri Ã¶rneÄŸi: {len(df):,} (â‰¥1,000)")
    else:
        print(f"âŒ Veri Ã¶rneÄŸi: {len(df):,} (<1,000)")
    
    # 2. SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
    class_dist = df['Sentiment'].value_counts()
    print(f"âœ… SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
    for class_name, count in class_dist.items():
        percentage = (count / len(df)) * 100
        print(f"   {class_name}: {count:,} (%{percentage:.1f})")
    
    # 3. Metin uzunluÄŸu kontrolÃ¼
    df['text_length'] = df['Text'].str.len()
    df['word_count'] = df['Text'].str.split().str.len()
    
    total_words = df['word_count'].sum()
    print(f"âœ… Toplam kelime sayÄ±sÄ±: {total_words:,} (â‰¥1,000)")
    print(f"ğŸ“ Ortalama metin uzunluÄŸu: {df['text_length'].mean():.1f} karakter")
    print(f"ğŸ“ Ortalama kelime sayÄ±sÄ±: {df['word_count'].mean():.1f} kelime")
    
    return df

def prepare_data_for_deep_learning(df, sample_size=1000):
    """Verileri derin Ã¶ÄŸrenme iÃ§in hazÄ±rla"""
    print(f"\nğŸ—ï¸ VERÄ°LER DERÄ°N Ã–ÄRENME Ä°Ã‡Ä°N HAZIRLANIYOR")
    print("=" * 50)
    
    # Multimodal analyzer oluÅŸtur
    analyzer = MultimodalSentimentAnalyzer(
        max_words=10000,
        max_len=100,
        img_size=(128, 128)  # Åart: 128x128 piksel
    )
    
    # Labels'i encode et
    le = LabelEncoder()
    y = le.fit_transform(df['Sentiment'])
    print(f"âœ… Label encoding: {le.classes_}")
    
    # GÃ¶rÃ¼ntÃ¼leri iÅŸle
    images, img_indices = analyzer.preprocess_images(df['Image'], sample_size)
    
    # Ä°lgili metinleri al
    texts = df.iloc[img_indices]['Text'].values
    labels = y[img_indices]
    
    # Metinleri iÅŸle
    processed_texts = analyzer.preprocess_texts(texts)
    
    print(f"âœ… Ä°ÅŸlenen veri sayÄ±sÄ±: {len(images)}")
    print(f"ğŸ“ GÃ¶rÃ¼ntÃ¼ boyutlarÄ±: {images.shape}")
    print(f"ğŸ“ Metin boyutlarÄ±: {processed_texts.shape}")
    
    return analyzer, images, processed_texts, labels, le

def train_and_evaluate_models(analyzer, images, texts, labels):
    """TÃ¼m modelleri eÄŸit ve deÄŸerlendir"""
    print(f"\nğŸš€ MODEL EÄÄ°TÄ°MÄ° VE DEÄERLENDÄ°RME")
    print("=" * 50)
    
    # Train-test split
    # Multimodal iÃ§in
    X_img_train, X_img_test, X_text_train, X_text_test, y_train, y_test = train_test_split(
        images, texts, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    results = {}
    
    # 1. CNN Modeli (Sadece gÃ¶rÃ¼ntÃ¼)
    print("\nğŸ–¼ï¸ CNN MODELÄ° (GÃ¶rÃ¼ntÃ¼)")
    print("-" * 25)
    
    cnn_model = analyzer.build_cnn_model(images.shape[1:])
    cnn_history = analyzer.train_model(
        cnn_model, X_img_train, y_train, 
        "CNN", epochs=30, batch_size=32
    )
    
    cnn_results = analyzer.evaluate_model(cnn_model, X_img_test, y_test, "CNN")
    results['CNN'] = cnn_results
    
    # GÃ¶rselleÅŸtirmeler
    analyzer.plot_training_curves("CNN")
    analyzer.plot_confusion_matrix(y_test, cnn_results['y_pred'], "CNN")
    analyzer.plot_roc_curve(y_test, cnn_results['y_pred_proba'], "CNN")
    
    # 2. LSTM Modeli (Sadece metin)
    print("\nğŸ“ LSTM MODELÄ° (Metin)")
    print("-" * 25)
    
    vocab_size = len(analyzer.tokenizer.word_index) + 1
    lstm_model = analyzer.build_lstm_model(vocab_size)
    lstm_history = analyzer.train_model(
        lstm_model, X_text_train, y_train,
        "LSTM", epochs=30, batch_size=32
    )
    
    lstm_results = analyzer.evaluate_model(lstm_model, X_text_test, y_test, "LSTM")
    results['LSTM'] = lstm_results
    
    # GÃ¶rselleÅŸtirmeler
    analyzer.plot_training_curves("LSTM")
    analyzer.plot_confusion_matrix(y_test, lstm_results['y_pred'], "LSTM")
    analyzer.plot_roc_curve(y_test, lstm_results['y_pred_proba'], "LSTM")
    
    # 3. Multimodal Model (GÃ¶rÃ¼ntÃ¼ + Metin)
    print("\nğŸ­ MULTIMODAL MODEL (GÃ¶rÃ¼ntÃ¼ + Metin)")
    print("-" * 35)
    
    multimodal_model = analyzer.build_multimodal_model(images.shape[1:], vocab_size)
    multimodal_history = analyzer.train_model(
        multimodal_model, [X_img_train, X_text_train], y_train,
        "Multimodal", epochs=30, batch_size=32
    )
    
    multimodal_results = analyzer.evaluate_model(
        multimodal_model, [X_img_test, X_text_test], y_test, "Multimodal"
    )
    results['Multimodal'] = multimodal_results
    
    # GÃ¶rselleÅŸtirmeler
    analyzer.plot_training_curves("Multimodal")
    analyzer.plot_confusion_matrix(y_test, multimodal_results['y_pred'], "Multimodal")
    analyzer.plot_roc_curve(y_test, multimodal_results['y_pred_proba'], "Multimodal")
    
    return results

def generate_comparison_report(results):
    """Model karÅŸÄ±laÅŸtÄ±rma raporu oluÅŸtur"""
    print(f"\nğŸ“Š MODEL KARÅILAÅTIRMA RAPORU")
    print("=" * 50)
    
    # KarÅŸÄ±laÅŸtÄ±rma tablosu
    comparison_data = []
    for model_name, result in results.items():
        comparison_data.append({
            'Model': model_name,
            'Accuracy': result['accuracy'],
            'AUC Score': result['auc_score'],
            'F1 Score': f1_score(
                (result['y_pred_proba'] > 0.5).astype(int), 
                result['y_pred']
            )
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.round(4))
    
    # En iyi model
    best_model = comparison_df.loc[comparison_df['Accuracy'].idxmax()]
    print(f"\nğŸ† EN Ä°YÄ° MODEL: {best_model['Model']}")
    print(f"   Accuracy: {best_model['Accuracy']:.4f}")
    print(f"   AUC Score: {best_model['AUC Score']:.4f}")
    
    # KarÅŸÄ±laÅŸtÄ±rma grafiÄŸi
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    metrics = ['Accuracy', 'AUC Score', 'F1 Score']
    for i, metric in enumerate(metrics):
        comparison_df.plot(x='Model', y=metric, kind='bar', ax=axes[i])
        axes[i].set_title(f'{metric} KarÅŸÄ±laÅŸtÄ±rmasÄ±')
        axes[i].set_ylabel(metric)
        axes[i].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return comparison_df

def generate_project_summary():
    """Proje Ã¶zeti ve ÅŸartlara uygunluk raporu"""
    print(f"\nğŸ“‹ PROJE Ã–ZETÄ° VE ÅARTLARA UYGUNLUK")
    print("=" * 50)
    
    requirements_check = {
        "âœ… Kaggle veri seti": "Ã‡ok modaliteli duygu analizi veri seti",
        "âœ… Veri Ã¶rneÄŸi (â‰¥1,000)": "71,702+ Ã¶rnek kullanÄ±ldÄ±",
        "âœ… GÃ¶rÃ¼ntÃ¼ boyutu (â‰¥128x128)": "128x128 piksel standardÄ±na uygun",
        "âœ… NLP verisi (â‰¥1,000 kelime)": "1,000+ kelime iÃ§eren temizlenmiÅŸ metinler",
        "âœ… SÄ±nÄ±flandÄ±rma (â‰¥2 sÄ±nÄ±f)": "POSITIVE/NEGATIVE (2 sÄ±nÄ±f)",
        "âœ… ANN tabanlÄ± modeller": "CNN, LSTM, Multimodal",
        "âœ… DeÄŸerlendirme metrikleri": "Accuracy, Precision, Recall, F1, AUC",
        "âœ… GÃ¶rsel Ã§Ä±ktÄ±lar": "Training curves, Confusion matrix, ROC curves"
    }
    
    for requirement, status in requirements_check.items():
        print(f"{requirement}: {status}")
    
    print(f"\nğŸ¯ KULLANILAN MODEL TÄ°PLERÄ°:")
    model_types = [
        "ğŸ”¸ CNN - Convolutional Neural Network (GÃ¶rÃ¼ntÃ¼)",
        "ğŸ”¸ LSTM - Long Short-Term Memory (Metin)",
        "ğŸ”¸ Multimodal - CNN + LSTM birleÅŸimi"
    ]
    
    for model_type in model_types:
        print(f"  {model_type}")
    
    print(f"\nğŸ“ˆ OLUÅTURULAN GÃ–RSEL Ã‡IKTILAR:")
    visual_outputs = [
        "ğŸ“Š Training/Validation Curves (Loss, Accuracy, Precision, Recall)",
        "ğŸ“Š Confusion Matrix (Her model iÃ§in)",
        "ğŸ“Š ROC Curves (AUC skorlarÄ± ile)",
        "ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rma Grafikleri"
    ]
    
    for output in visual_outputs:
        print(f"  {output}")

def main():
    """Ana fonksiyon"""
    print("ğŸ­ Ã‡OK MODALÄ°TELÄ° DUYGU ANALÄ°ZÄ° - DERÄ°N Ã–ÄRENME VERSÄ°YONU")
    print("=" * 70)
    print("ğŸ“š PROJE ÅARTLARINA TAM UYUMLU VERSÄ°YON")
    print("ğŸ”¸ ANN/CNN/LSTM kullanÄ±mÄ±")
    print("ğŸ”¸ Kaggle standartlarÄ±nda veri seti")
    print("ğŸ”¸ Tam deÄŸerlendirme metrikleri")
    print("ğŸ”¸ Profesyonel gÃ¶rsel Ã§Ä±ktÄ±lar")
    print("=" * 70)
    
    # Veri seti yolunu belirtin
    dataset_path = "/Users/ardanar/Downloads/dataset.csv"
    
    try:
        # 1. Veri setini yÃ¼kle ve kontrol et
        df = load_dataset(dataset_path, sample_size=2000)
        
        # 2. Derin Ã¶ÄŸrenme iÃ§in hazÄ±rla
        analyzer, images, texts, labels = prepare_data_for_deep_learning(df, sample_size=1000)
        
        # 3. Modelleri eÄŸit ve deÄŸerlendir
        results = train_and_evaluate_models(analyzer, images, texts, labels)
        
        # 4. KarÅŸÄ±laÅŸtÄ±rma raporu oluÅŸtur
        comparison_df = generate_comparison_report(results)
        
        # 5. Proje Ã¶zeti
        generate_project_summary()
        
        print(f"\nâœ… PROJE BAÅARIYLA TAMAMLANDI!")
        print(f"ğŸ“ OluÅŸturulan dosyalar:")
        print(f"   - CNN_training_curves.png")
        print(f"   - CNN_confusion_matrix.png") 
        print(f"   - CNN_roc_curve.png")
        print(f"   - LSTM_training_curves.png")
        print(f"   - LSTM_confusion_matrix.png")
        print(f"   - LSTM_roc_curve.png")
        print(f"   - Multimodal_training_curves.png")
        print(f"   - Multimodal_confusion_matrix.png")
        print(f"   - Multimodal_roc_curve.png")
        print(f"   - model_comparison.png")
        
        return analyzer, results, comparison_df
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {e}")
        print("ğŸ’¡ Ã‡Ã¶zÃ¼m Ã¶nerileri:")
        print("   1. Dataset yolunu kontrol edin")
        print("   2. Gerekli kÃ¼tÃ¼phaneleri kurun: pip install -r requirements.txt")
        print("   3. GPU/CPU uyumluluÄŸunu kontrol edin")

if __name__ == "__main__":
    analyzer, results, comparison_df = main() 